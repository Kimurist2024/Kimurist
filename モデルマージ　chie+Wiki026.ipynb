{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimurist2024/Kimurist/blob/main/%E3%83%A2%E3%83%87%E3%83%AB%E3%83%9E%E3%83%BC%E3%82%B8%E3%80%80chie%2BWiki026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUVux0bIuzzw",
        "outputId": "f5091014-8afe-4a61-a3e1-6865f738bae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mergekit'...\n",
            "remote: Enumerating objects: 2685, done.\u001b[K\n",
            "remote: Counting objects: 100% (994/994), done.\u001b[K\n",
            "remote: Compressing objects: 100% (277/277), done.\u001b[K\n",
            "remote: Total 2685 (delta 865), reused 717 (delta 717), pack-reused 1691 (from 4)\u001b[K\n",
            "Receiving objects: 100% (2685/2685), 922.36 KiB | 12.14 MiB/s, done.\n",
            "Resolving deltas: 100% (1825/1825), done.\n",
            "Obtaining file:///content/mergekit\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: click==8.1.8 in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (8.1.8)\n",
            "Requirement already satisfied: safetensors~=0.5.2 in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (0.5.2)\n",
            "Requirement already satisfied: accelerate~=1.3.0 in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: pydantic~=2.10.6 in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (2.10.6)\n",
            "Collecting immutables==0.20 (from mergekit==0.1.0)\n",
            "  Downloading immutables-0.20-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (4.48.3)\n",
            "Requirement already satisfied: tokenizers>=0.20.1 in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (4.25.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mergekit==0.1.0) (1.13.1)\n",
            "Collecting datasets (from mergekit==0.1.0)\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate~=1.3.0->mergekit==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate~=1.3.0->mergekit==0.1.0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate~=1.3.0->mergekit==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate~=1.3.0->mergekit==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->mergekit==0.1.0) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->mergekit==0.1.0) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->mergekit==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.10.6->mergekit==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.10.6->mergekit==0.1.0) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->mergekit==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->mergekit==0.1.0) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->mergekit==0.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->mergekit==0.1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->mergekit==0.1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->mergekit==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->mergekit==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->mergekit==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.2->mergekit==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->mergekit==0.1.0) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->mergekit==0.1.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->mergekit==0.1.0) (2.2.2)\n",
            "Collecting xxhash (from datasets->mergekit==0.1.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->mergekit==0.1.0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->mergekit==0.1.0) (3.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mergekit==0.1.0) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mergekit==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mergekit==0.1.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mergekit==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mergekit==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mergekit==0.1.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->mergekit==0.1.0) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->mergekit==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->mergekit==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->mergekit==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->mergekit==0.1.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->mergekit==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->mergekit==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->mergekit==0.1.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->mergekit==0.1.0) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->mergekit==0.1.0) (1.17.0)\n",
            "Downloading immutables-0.20-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.1-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mergekit\n",
            "  Building editable for mergekit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mergekit: filename=mergekit-0.1.0-0.editable-py3-none-any.whl size=12528 sha256=bcb350d525169cacca6a02acc0481a947793da55bf47e275f64733e2800bc08c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bqns8fgr/wheels/e6/32/13/7a04b3a97b71d70ab7b90aa1e78940618364a0521ccb6532d0\n",
            "Successfully built mergekit\n",
            "Installing collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, immutables, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, mergekit\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed datasets-3.3.1 dill-0.3.8 immutables-0.20 mergekit-0.1.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/arcee-ai/mergekit.git\n",
        "!pip install -e ./mergekit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p examples"
      ],
      "metadata": {
        "id": "g836Gj28vEBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile examples/llm-jp-linear.yaml\n",
        "models:\n",
        "  - model: \"model1.safetensors\"\n",
        "    parameters:\n",
        "      weight: 0.7\n",
        "  - model: \"model2.safetensors\"\n",
        "    parameters:\n",
        "      weight: 0.3\n",
        "merge_method: linear\n",
        "dtype: float16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTl1V922vYlM",
        "outputId": "9fd96b07-523a-40c1-adb5-5dc714c47102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing examples/llm-jp-linear.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade safetensors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAlj0dYHvbG1",
        "outputId": "8b482033-5a87-4089-d048-3b87275d6879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def create_tokenizer_config(path=\"/content/sample_data/tokenizer_config.json\"):\n",
        "    # トークナイザーの設定\n",
        "    config = {\n",
        "        \"tokenizer_class\": \"AutoTokenizer\",\n",
        "        \"model_max_length\": 512,\n",
        "        \"padding_side\": \"right\",\n",
        "        \"truncation_side\": \"right\",\n",
        "        \"special_tokens_map\": {\n",
        "            \"bos_token\": \"<s>\",\n",
        "            \"eos_token\": \"</s>\",\n",
        "            \"unk_token\": \"<unk>\",\n",
        "            \"pad_token\": \"<pad>\",\n",
        "            \"mask_token\": \"<mask>\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # ディレクトリ作成（必要な場合）\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "\n",
        "    # JSONファイルを書き出し\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"{path} を作成しました\")\n",
        "\n",
        "# 実行\n",
        "create_tokenizer_config()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BU9sfHv2uu1",
        "outputId": "e765ba08-640d-422f-d5e5-ce8fc155358b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sample_data/tokenizer_config.json を作成しました\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqrhp4XpYMPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "def create_merge_config():\n",
        "    \"\"\"merge_config.ymlを作成する\"\"\"\n",
        "    config_content = \"\"\"\n",
        "merge_method: linear\n",
        "models:\n",
        "  - model:\n",
        "      path: \"/content/sample_data/model1.safetensors\"\n",
        "    alpha: 0.6\n",
        "  - model:\n",
        "      path: \"/content/sample_data/model2.safetensors\"\n",
        "    alpha: 0.4\n",
        "tokenizer_source: \"/content/sample_data\"\n",
        "dtype: float16\n",
        "\"\"\"\n",
        "    with open('merge_config.yml', 'w') as f:\n",
        "        f.write(config_content)\n",
        "    print(\" merge_config.yml を作成しました\")\n",
        "\n",
        "def run_mergekit():\n",
        "    \"\"\"MergeKitでモデルをマージする\"\"\"\n",
        "    config_path = \"merge_config.yml\"\n",
        "    output_dir = \"./merged_model\"\n",
        "\n",
        "    command = [\n",
        "        \"mergekit-yaml\", config_path, output_dir, \"--cuda\", \"--lazy-unpickle\", \"--allow-crimes\"\n",
        "    ]\n",
        "\n"
      ],
      "metadata": {
        "id": "FGmavoTW69uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "def inspect_safetensors(model_path):\n",
        "    \"\"\"safetensorsファイル内のテンソル情報を表示\"\"\"\n",
        "    try:\n",
        "        tensors = load_file(model_path)\n",
        "        print(f\"\\n モデル: {model_path}\")\n",
        "        print(\"=\"*50)\n",
        "        for key, tensor in tensors.items():\n",
        "            print(f\"🔹 {key}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        print(f\" 読み込みエラー: {model_path}\\n{e}\")\n",
        "\n",
        "# 実行\n",
        "inspect_safetensors(\"model1.safetensors\")\n",
        "inspect_safetensors(\"model2.safetensors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf-fm1Tv_GzK",
        "outputId": "c8a74be3-9bcc-4cbd-9a37-8d748b49bb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 読み込みエラー: model1.safetensors\n",
            "Error while deserializing header: MetadataIncompleteBuffer\n",
            "\n",
            " モデル: model2.safetensors\n",
            "==================================================\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: shape=torch.Size([32, 8]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.final_layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: shape=torch.Size([32, 8]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.final_layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 shared.weight: shape=torch.Size([32128, 512]), dtype=torch.float32\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "def inspect_safetensors(model_path):\n",
        "    \"\"\"safetensorsファイル内のテンソル情報を表示\"\"\"\n",
        "    try:\n",
        "        tensors = load_file(model_path)\n",
        "        print(f\"\\n モデル: {model_path}\")\n",
        "        print(\"=\"*50)\n",
        "        for key, tensor in tensors.items():\n",
        "            print(f\"🔹 {key}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        print(f\" 読み込みエラー: {model_path}\\n{e}\")\n",
        "\n",
        "# 実行\n",
        "inspect_safetensors(\"model1.safetensors\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-RdAXsD_Lnj",
        "outputId": "5a9ae462-e253-4820-cb60-d1e75cf21641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " モデル: model1.safetensors\n",
            "==================================================\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: shape=torch.Size([32, 12]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.10.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.11.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.6.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.7.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.8.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.9.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 decoder.final_layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: shape=torch.Size([32, 12]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.10.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.10.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.10.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.10.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.10.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.10.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.10.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.10.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.10.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.11.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.11.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.11.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.11.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.11.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.11.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.11.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.11.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.11.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.6.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.6.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.6.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.6.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.6.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.6.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.6.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.6.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.6.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.7.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.7.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.7.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.7.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.7.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.7.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.7.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.7.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.7.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.8.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.8.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.8.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.8.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.8.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.8.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.8.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.8.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.8.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.9.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.9.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.9.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.9.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.9.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.block.9.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.9.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "🔹 encoder.block.9.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.9.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 encoder.final_layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "🔹 lm_head.weight: shape=torch.Size([32128, 768]), dtype=torch.float32\n",
            "🔹 shared.weight: shape=torch.Size([32128, 768]), dtype=torch.float32\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "def inspect_safetensors(model_path):\n",
        "    \"\"\"safetensorsファイル内のテンソル情報を表示\"\"\"\n",
        "    try:\n",
        "        tensors = load_file(model_path)\n",
        "        print(f\"\\n モデル: {model_path}\")\n",
        "        print(\"=\"*50)\n",
        "        for key, tensor in tensors.items():\n",
        "            print(f\"🔹 {key}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        print(f\" 読み込みエラー: {model_path}\\n{e}\")\n",
        "\n",
        "# 実行\n",
        "inspect_safetensors(\"model2.safetensors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuL_FDTL_O1A",
        "outputId": "37e5478f-3593-4405-a47c-b93c33d796ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " モデル: model2.safetensors\n",
            "==================================================\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: shape=torch.Size([32, 8]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.0.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.1.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.2.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.3.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.4.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.EncDecAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 decoder.block.5.layer.2.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 decoder.final_layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: shape=torch.Size([32, 8]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.0.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.1.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.2.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.3.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.4.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.k.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.o.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.q.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.SelfAttention.v.weight: shape=torch.Size([512, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.0.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.DenseReluDense.wi.weight: shape=torch.Size([2048, 512]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.DenseReluDense.wo.weight: shape=torch.Size([512, 2048]), dtype=torch.float32\n",
            "🔹 encoder.block.5.layer.1.layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 encoder.final_layer_norm.weight: shape=torch.Size([512]), dtype=torch.float32\n",
            "🔹 shared.weight: shape=torch.Size([32128, 512]), dtype=torch.float32\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_merge_config(model1_path, model2_path, output_yaml=\"merge_config.yml\", w1=0.6, w2=0.4):\n",
        "    config = f\"\"\"\n",
        "merge_method: linear\n",
        "models:\n",
        "  - model:\n",
        "      path: \"{model1_path}\"\n",
        "    alpha: {w1}\n",
        "  - model:\n",
        "      path: \"{model2_path}\"\n",
        "    alpha: {w2}\n",
        "tokenizer_source: \"/content/sample_data\"\n",
        "dtype: float16\n",
        "\"\"\"\n",
        "    with open(output_yaml, \"w\") as f:\n",
        "        f.write(config)\n",
        "    print(f\"✅ merge_config.yml を生成しました: {output_yaml}\")\n",
        "\n",
        "# 実行\n",
        "create_merge_config(\"model1.safetensors\", \"model2.safetensors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x7O-0Ju0o7f",
        "outputId": "4f666eef-483b-4535-9f7f-2f13a8714c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ merge_config.yml を生成しました: merge_config.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from safetensors.torch import load_file, save_file\n",
        "import subprocess\n",
        "\n",
        "def merge_safetensors(model1_path, model2_path, output_path=\"merged_model.safetensors\", w1=0.6, w2=0.4):\n",
        "    # モデルを読み込む\n",
        "    m1, m2 = load_file(model1_path), load_file(model2_path)\n",
        "    merged = {}\n",
        "\n",
        "    # model1 のテンソルを基準に model2 を調整\n",
        "    for k, v1 in m1.items():\n",
        "        t2 = m2.get(k, torch.zeros_like(v1))  # model2 にキーがない場合はゼロ埋め\n",
        "        t2 = t2.to(v1.dtype)  # データ型を統一\n",
        "\n",
        "        # 形状調整\n",
        "        if v1.shape != t2.shape:\n",
        "            print(f\"🔧 {k}: {t2.shape} → {v1.shape} に調整中\")\n",
        "\n",
        "            if len(v1.shape) == 1:\n",
        "                t2 = torch.nn.functional.pad(t2, (0, max(0, v1.shape[0] - t2.shape[0])))[:v1.shape[0]]\n",
        "\n",
        "            elif len(v1.shape) == 2 and t2.numel() == v1.numel():\n",
        "                t2 = t2.reshape(v1.shape)\n",
        "\n",
        "            elif len(v1.shape) > 2 and t2.numel() == v1.numel():\n",
        "                t2 = t2.reshape(v1.shape)\n",
        "\n",
        "            else:\n",
        "                print(f\"⚠️ {k}: 形状不一致のためランダム初期化\")\n",
        "                t2 = torch.randn(v1.shape, dtype=v1.dtype)\n",
        "\n",
        "        # 指定した比率でモデルをマージ\n",
        "        merged[k] = w1 * v1 + w2 * t2\n",
        "\n",
        "    # マージ結果を保存\n",
        "    save_file(merged, output_path)\n",
        "    print(f\"✅ モデルマージ完了: {output_path}\")\n",
        "\n",
        "    # `merge_config.yml` を生成\n",
        "    config = f\"\"\"\n",
        "merge_method: linear\n",
        "models:\n",
        "  - model:\n",
        "      path: \"{model1_path}\"\n",
        "    alpha: {w1}\n",
        "  - model:\n",
        "      path: \"{model2_path}\"\n",
        "    alpha: {w2}\n",
        "tokenizer_source: \"/content/sample_data\"\n",
        "dtype: float16\n",
        "\"\"\"\n",
        "    with open(\"merge_config.yml\", \"w\") as f:\n",
        "        f.write(config)\n",
        "    print(\"✅ merge_config.yml を生成しました\")\n",
        "\n",
        "    # `mergekit` でマージを実行\n",
        "    subprocess.run([\"mergekit-yaml\", \"merge_config.yml\", output_path, \"--cuda\", \"--lazy-unpickle\", \"--allow-crimes\"])\n",
        "    print(f\"✅ mergekit による最終マージ完了: {output_path}\")\n",
        "\n",
        "# 実行\n",
        "merge_safetensors(\"model1.safetensors\", \"model2.safetensors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV8Q9wtW005B",
        "outputId": "95719b52-9fab-442c-8bc5-978f7d6d3e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 decoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 8]) → torch.Size([32, 12]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.0.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.0.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.0.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.1.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.1.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.1.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.2.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.2.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.2.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.3.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.3.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.3.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.4.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.4.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.4.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.5.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.5.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.5.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.final_layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 8]) → torch.Size([32, 12]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.0.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.0.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.1.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.1.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.1.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.1.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.1.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.1.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.2.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.2.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.2.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.2.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.2.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.2.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.3.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.3.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.3.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.3.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.3.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.3.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.4.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.4.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.4.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.4.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.4.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.4.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.5.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.5.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.5.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.5.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.5.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.5.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.final_layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 shared.weight: torch.Size([32128, 512]) → torch.Size([32128, 768]) に調整中\n",
            "⚠️ shared.weight: 形状不一致のためランダム初期化\n",
            "✅ モデルマージ完了: merged_model.safetensors\n",
            "✅ merge_config.yml を生成しました\n",
            "✅ mergekit による最終マージ完了: merged_model.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from safetensors.torch import load_file, save_file\n",
        "import subprocess\n",
        "import json\n",
        "import os\n",
        "\n",
        "def merge_safetensors(model1_path, model2_path, output_path=\"merged_model.safetensors\", w1=0.6, w2=0.4):\n",
        "    # モデルを読み込む\n",
        "    m1, m2 = load_file(model1_path), load_file(model2_path)\n",
        "    merged = {}\n",
        "\n",
        "    # model1 のテンソルを基準に model2 を調整\n",
        "    for k, v1 in m1.items():\n",
        "        t2 = m2.get(k, torch.zeros_like(v1))\n",
        "        t2 = t2.to(v1.dtype)\n",
        "\n",
        "        if v1.shape != t2.shape:\n",
        "            print(f\"🔧 {k}: {t2.shape} → {v1.shape} に調整中\")\n",
        "\n",
        "            if len(v1.shape) == 1:\n",
        "                t2 = torch.nn.functional.pad(t2, (0, max(0, v1.shape[0] - t2.shape[0])))[:v1.shape[0]]\n",
        "            elif len(v1.shape) == 2 and t2.numel() == v1.numel():\n",
        "                t2 = t2.reshape(v1.shape)\n",
        "            elif len(v1.shape) > 2 and t2.numel() == v1.numel():\n",
        "                t2 = t2.reshape(v1.shape)\n",
        "            else:\n",
        "                print(f\"⚠️ {k}: 形状不一致のためランダム初期化\")\n",
        "                t2 = torch.randn(v1.shape, dtype=v1.dtype)\n",
        "\n",
        "        merged[k] = w1 * v1 + w2 * t2\n",
        "\n",
        "    # マージ結果を保存\n",
        "    save_file(merged, output_path)\n",
        "    print(f\"✅ モデルマージ完了: {output_path}\")\n",
        "\n",
        "    # `merge_config.yml` を生成\n",
        "    config_yml = f\"\"\"\n",
        "merge_method: linear\n",
        "models:\n",
        "  - model:\n",
        "      path: \"{model1_path}\"\n",
        "    alpha: {w1}\n",
        "  - model:\n",
        "      path: \"{model2_path}\"\n",
        "    alpha: {w2}\n",
        "tokenizer_source: \"/content/sample_data\"\n",
        "dtype: float16\n",
        "\"\"\"\n",
        "    with open(\"merge_config.yml\", \"w\") as f:\n",
        "        f.write(config_yml)\n",
        "    print(\"✅ merge_config.yml を作成しました\")\n",
        "\n",
        "    # `mergekit` でマージを実行\n",
        "    subprocess.run([\"mergekit-yaml\", \"merge_config.yml\", output_path, \"--cuda\", \"--lazy-unpickle\", \"--allow-crimes\"])\n",
        "    print(f\"✅ mergekit による最終マージ完了: {output_path}\")\n",
        "\n",
        "    # **✅ `config.json` を生成**\n",
        "    generate_config(output_path)\n",
        "\n",
        "def generate_config(model_path, config_path=\"merged_model/config.json\"):\n",
        "    tensors = load_file(model_path)\n",
        "\n",
        "    # ディレクトリが存在しない場合は作成\n",
        "    config_dir = os.path.dirname(config_path)\n",
        "    if config_dir and not os.path.exists(config_dir):\n",
        "        os.makedirs(config_dir)\n",
        "\n",
        "    # 隠れ層サイズ (hidden_size)\n",
        "    hidden_size = tensors[\"decoder.block.0.layer.0.SelfAttention.k.weight\"].shape[0]\n",
        "\n",
        "    # レイヤー数 (num_hidden_layers) をカウント\n",
        "    num_hidden_layers = max(\n",
        "        int(k.split(\".\")[2]) for k in tensors.keys() if \"decoder.block\" in k\n",
        "    ) + 1  # 最大の X + 1\n",
        "\n",
        "    # アテンションヘッド数 (num_attention_heads)\n",
        "    num_attention_heads = tensors[\"decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\"].shape[1]\n",
        "\n",
        "    # FFN の中間層サイズ (intermediate_size)\n",
        "    intermediate_size = tensors[\"decoder.block.0.layer.2.DenseReluDense.wi_0.weight\"].shape[0]\n",
        "\n",
        "    # 語彙サイズ (vocab_size)\n",
        "    vocab_size = tensors[\"shared.weight\"].shape[0]\n",
        "\n",
        "    # `config.json` の構造\n",
        "    config = {\n",
        "        \"model_type\": \"transformer\",\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_attention_heads\": num_attention_heads,\n",
        "        \"num_hidden_layers\": num_hidden_layers,\n",
        "        \"intermediate_size\": intermediate_size,\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"max_position_embeddings\": 1024,  # 一般的な値（要確認）\n",
        "        \"dtype\": str(tensors[\"decoder.block.0.layer.0.SelfAttention.k.weight\"].dtype)\n",
        "    }\n",
        "\n",
        "    # JSONファイルとして保存\n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    print(f\"✅ `config.json` を作成しました: {config_path}\")\n",
        "\n",
        "# 実行\n",
        "merge_safetensors(\"model1.safetensors\", \"model2.safetensors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxl4yp2OB-rx",
        "outputId": "c984cc7b-afd1-4b3e-a466-47f897baaff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 decoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 8]) → torch.Size([32, 12]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.0.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.0.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.0.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.0.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.0.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.1.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.1.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.1.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.1.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.1.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.2.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.2.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.2.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.2.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.2.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.3.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.3.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.3.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.3.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.3.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.4.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.4.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.4.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.4.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.4.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.5.layer.1.EncDecAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.1.EncDecAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.1.EncDecAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.1.EncDecAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.1.EncDecAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.1.EncDecAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.1.EncDecAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ decoder.block.5.layer.1.EncDecAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.block.5.layer.2.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ decoder.block.5.layer.2.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 decoder.block.5.layer.2.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 decoder.final_layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 8]) → torch.Size([32, 12]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.0.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.0.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.0.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.0.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.1.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.1.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.1.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.1.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.1.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.1.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.1.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.2.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.2.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.2.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.2.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.2.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.2.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.2.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.3.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.3.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.3.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.3.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.3.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.3.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.3.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.4.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.4.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.4.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.4.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.4.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.4.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.4.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.5.layer.0.SelfAttention.k.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.5.layer.0.SelfAttention.o.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.5.layer.0.SelfAttention.q.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([512, 512]) → torch.Size([768, 768]) に調整中\n",
            "⚠️ encoder.block.5.layer.0.SelfAttention.v.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.0.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.block.5.layer.1.DenseReluDense.wo.weight: torch.Size([512, 2048]) → torch.Size([768, 2048]) に調整中\n",
            "⚠️ encoder.block.5.layer.1.DenseReluDense.wo.weight: 形状不一致のためランダム初期化\n",
            "🔧 encoder.block.5.layer.1.layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 encoder.final_layer_norm.weight: torch.Size([512]) → torch.Size([768]) に調整中\n",
            "🔧 shared.weight: torch.Size([32128, 512]) → torch.Size([32128, 768]) に調整中\n",
            "⚠️ shared.weight: 形状不一致のためランダム初期化\n",
            "✅ モデルマージ完了: merged_model.safetensors\n",
            "✅ merge_config.yml を作成しました\n",
            "✅ mergekit による最終マージ完了: merged_model.safetensors\n",
            "✅ `config.json` を作成しました: merged_model/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(\"merged_model/config.json\"))  # True なら成功\n",
        "\n",
        "# `config.json` の内容を表示\n",
        "with open(\"merged_model/config.json\", \"r\") as f:\n",
        "    print(json.load(f))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RNNETmICTzv",
        "outputId": "4327418d-dcd2-4ec8-d6f9-991fa7200e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "{'model_type': 'transformer', 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'intermediate_size': 2048, 'vocab_size': 32128, 'max_position_embeddings': 1024, 'dtype': 'torch.float32'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"merged_model/config.json\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aLV54Cc-CroA",
        "outputId": "813e78b6-19c7-45d5-998a-d183f24a3285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3a532976-24df-44b1-8423-1eb496f456e5\", \"config.json\", 242)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# safetensors の中身を確認\n",
        "tensors = load_file(\"merged_model.safetensors\")\n",
        "\n",
        "# テンソルのキー一覧を表示\n",
        "for key, tensor in tensors.items():\n",
        "    print(f\"{key}: shape={tensor.shape}, dtype={tensor.dtype}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIaBcHH95RlH",
        "outputId": "de9f9321-49cd-409c-db08-76b58a36f38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder.block.0.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: shape=torch.Size([32, 12]), dtype=torch.float32\n",
            "decoder.block.0.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.0.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.0.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.0.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.0.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.1.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.1.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.1.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.1.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.1.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.10.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.10.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.10.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.10.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.10.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.11.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.11.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.11.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.11.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.11.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.2.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.2.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.2.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.2.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.2.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.3.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.3.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.3.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.3.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.3.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.4.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.4.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.4.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.4.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.4.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.5.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.5.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.5.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.5.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.5.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.6.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.6.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.6.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.6.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.6.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.7.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.7.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.7.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.7.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.7.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.8.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.8.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.8.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.8.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.8.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.9.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.9.layer.1.EncDecAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.1.EncDecAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.1.EncDecAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.1.EncDecAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.block.9.layer.2.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.2.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "decoder.block.9.layer.2.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "decoder.block.9.layer.2.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "decoder.final_layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.0.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.0.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.0.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: shape=torch.Size([32, 12]), dtype=torch.float32\n",
            "encoder.block.0.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.0.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.0.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.0.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.0.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.0.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.1.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.1.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.1.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.1.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.1.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.1.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.1.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.1.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.1.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.10.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.10.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.10.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.10.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.10.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.10.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.10.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.10.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.10.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.11.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.11.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.11.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.11.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.11.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.11.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.11.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.11.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.11.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.2.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.2.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.2.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.2.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.2.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.2.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.2.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.2.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.2.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.3.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.3.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.3.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.3.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.3.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.3.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.3.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.3.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.3.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.4.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.4.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.4.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.4.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.4.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.4.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.4.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.4.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.4.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.5.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.5.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.5.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.5.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.5.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.5.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.5.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.5.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.5.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.6.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.6.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.6.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.6.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.6.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.6.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.6.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.6.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.6.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.7.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.7.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.7.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.7.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.7.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.7.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.7.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.7.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.7.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.8.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.8.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.8.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.8.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.8.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.8.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.8.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.8.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.8.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.9.layer.0.SelfAttention.k.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.9.layer.0.SelfAttention.o.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.9.layer.0.SelfAttention.q.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.9.layer.0.SelfAttention.v.weight: shape=torch.Size([768, 768]), dtype=torch.float32\n",
            "encoder.block.9.layer.0.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.block.9.layer.1.DenseReluDense.wi_0.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.9.layer.1.DenseReluDense.wi_1.weight: shape=torch.Size([2048, 768]), dtype=torch.float32\n",
            "encoder.block.9.layer.1.DenseReluDense.wo.weight: shape=torch.Size([768, 2048]), dtype=torch.float32\n",
            "encoder.block.9.layer.1.layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "encoder.final_layer_norm.weight: shape=torch.Size([768]), dtype=torch.float32\n",
            "lm_head.weight: shape=torch.Size([32128, 768]), dtype=torch.float32\n",
            "shared.weight: shape=torch.Size([32128, 768]), dtype=torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "def generate_config(model_path, config_path=\"merged_model/config.json\"):\n",
        "    tensors = load_file(model_path)\n",
        "\n",
        "    # 隠れ層サイズ (hidden_size)\n",
        "    hidden_size = tensors[\"decoder.block.0.layer.0.SelfAttention.k.weight\"].shape[0]\n",
        "\n",
        "    # レイヤー数 (num_hidden_layers) をカウント\n",
        "    num_hidden_layers = max(\n",
        "        int(k.split(\".\")[2]) for k in tensors.keys() if \"decoder.block\" in k\n",
        "    ) + 1  # 最大の X + 1\n",
        "\n",
        "    # アテンションヘッド数 (num_attention_heads)\n",
        "    num_attention_heads = tensors[\"decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\"].shape[1]\n",
        "\n",
        "    # FFN の中間層サイズ (intermediate_size)\n",
        "    intermediate_size = tensors[\"decoder.block.0.layer.2.DenseReluDense.wi_0.weight\"].shape[0]\n",
        "\n",
        "    # 語彙サイズ (vocab_size)\n",
        "    vocab_size = tensors[\"shared.weight\"].shape[0]\n",
        "\n",
        "    # `config.json` の構造\n",
        "    config = {\n",
        "        \"model_type\": \"transformer\",\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_attention_heads\": num_attention_heads,\n",
        "        \"num_hidden_layers\": num_hidden_layers,\n",
        "        \"intermediate_size\": intermediate_size,\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"max_position_embeddings\": 1024,  # 一般的な値（要確認）\n",
        "        \"dtype\": str(tensors[\"decoder.block.0.layer.0.SelfAttention.k.weight\"].dtype)\n",
        "    }\n",
        "\n",
        "    # JSONファイルとして保存\n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    print(f\"✅ `config.json` を作成しました: {config_path}\")\n",
        "\n",
        "# 実行\n",
        "generate_config(\"merged_model.safetensors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "aU97DZBq5pFw",
        "outputId": "c61dbe86-cefa-42e2-c1c9-bc87be4de1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'merged_model/config.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b0cbd90d4037>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# 実行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mgenerate_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merged_model.safetensors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-b0cbd90d4037>\u001b[0m in \u001b[0;36mgenerate_config\u001b[0;34m(model_path, config_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# JSONファイルとして保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'merged_model/config.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l\n",
        "# または\n",
        "!ls -l merged_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLIrUgADvkwN",
        "outputId": "b8355364-5561-4a5a-bac7-78ca0a96124e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2170672\n",
            "drwxr-xr-x 2 root root      4096 Feb 19 00:28 examples\n",
            "-rw-r--r-- 1 root root       212 Feb 19 00:44 merge_config.yml\n",
            "drwxr-xr-x 9 root root      4096 Feb 19 00:19 mergekit\n",
            "-rw-r--r-- 1 root root 990345064 Feb 19 00:42 model1.safetensors\n",
            "-rw-r--r-- 1 root root 990345032 Feb 19 00:44 model2_aligned.safetensors\n",
            "-rw-r--r-- 1 root root 242041896 Feb 19 00:06 model2.safetensors\n",
            "drwxr-xr-x 1 root root      4096 Feb 19 00:28 sample_data\n",
            "ls: cannot access 'merged_model': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mergekit-yaml merge_config.yml /content/merged_model.safetensors --cuda --lazy-unpickle --allow-crimes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "694o0WPXVOYH",
        "outputId": "cf023a10-bce8-4e29-ace1-38a2ecec8794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-19 01:30:16.283590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739928616.305644   23021 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739928616.312112   23021 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-19 01:30:16.332790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/mergekit-yaml\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1161, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1082, in main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1443, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 788, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/mergekit/mergekit/options.py\", line 72, in wrapper\n",
            "    f(*args, **kwargs)\n",
            "  File \"/content/mergekit/mergekit/scripts/run_yaml.py\", line 32, in main\n",
            "    merge_config: MergeConfiguration = MergeConfiguration.model_validate(\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/main.py\", line 627, in model_validate\n",
            "    return cls.__pydantic_validator__.validate_python(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "pydantic_core._pydantic_core.ValidationError: 2 validation errors for MergeConfiguration\n",
            "models.0.model.model\n",
            "  Field required [type=missing, input_value={'path': 'model1.safetensors'}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
            "models.1.model.model\n",
            "  Field required [type=missing, input_value={'path': 'model2.safetensors'}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# マージ後のモデルをダウンロード\n",
        "files.download(\"/content/merged_model.safetensors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "a7Y8h4uFERmJ",
        "outputId": "9425fe05-aa7f-4543-9850-ecbbda9b220e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b5281ed0-0681-48a1-a173-7d800513a58d\", \"merged_model.safetensors\", 990345032)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!7z a \"/content/merged_model.7z\" \"/content/merged_model.safetensors\""
      ],
      "metadata": {
        "id": "h7W8O64JXRoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d33388-d5b0-4250-b915-c81c58cdee83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,8 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive:\n",
            "  0M Scan  /content/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 990345032 bytes (945 MiB)\n",
            "\n",
            "Creating archive: /content/merged_model.7z\n",
            "\n",
            "Items to compress: 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  0% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b100% 1 + merged_model.safetensors\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b100% 1 Header creation\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Files read from disk: 1\n",
            "Archive size: 909132400 bytes (868 MiB)\n",
            "Everything is Ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/merged_model.7z\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EG5ILda12NGt",
        "outputId": "3d119619-06f5-4b5f-d89e-13efe9c93a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9a1f36c-2d45-4c21-a33f-5eace3cc8faf\", \"merged_model.7z\", 909132400)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "# モデルをロード\n",
        "model_path = \"/content/merged_model.safetensors\"\n",
        "model = load_file(model_path)\n",
        "\n",
        "# モデル内のテンソル情報を表示\n",
        "print(f\"📊 モデル '{model_path}' 内のテンソル情報:\\n\")\n",
        "for k, v in model.items():\n",
        "    print(f\"🔹 {k}: shape={v.shape}, dtype={v.dtype}, mean={v.mean():.4f}, std={v.std():.4f}\")\n",
        "\n",
        "print(f\"\\n✅ {len(model)}個のテンソルがロードされました\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "LE097KpLFA_U",
        "outputId": "b6ef1b5f-af25-450e-b5d0-8dadcaf64059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: \"/content/merged_model.safetensors\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-65cc5e268333>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# モデルをロード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/merged_model.safetensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# モデル内のテンソル情報を表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \"\"\"\n\u001b[1;32m    312\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: \"/content/merged_model.safetensors\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "# モデルをロード\n",
        "model_path = \"/content/merged_model.safetensors\"\n",
        "model = load_file(model_path)\n",
        "\n",
        "# テンソルごとの推論速度測定\n",
        "start_time = time.time()\n",
        "for i, (k, v) in enumerate(model.items()):\n",
        "    _ = v * 2  # ダミー計算\n",
        "    if i >= 100:\n",
        "        break\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\" 100テンソルの計算時間: {end_time - start_time:.4f}秒\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1_hqA3fFcWX",
        "outputId": "05b3b4ee-fe35-4c62-a5c9-a23118a081c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100テンソルの計算時間: 0.0208秒\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5N6UQ-d5HQlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "# モデルをロード\n",
        "model_path = \"/content/model2.safetensors\"\n",
        "model = load_file(model_path)\n",
        "\n",
        "# テンソルごとの推論速度測定\n",
        "start_time = time.time()\n",
        "for i, (k, v) in enumerate(model.items()):\n",
        "    _ = v * 2  # ダミー計算\n",
        "    if i >= 100:\n",
        "        break\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\" 100テンソルの計算時間: {end_time - start_time:.4f}秒\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvRjZel0FrKW",
        "outputId": "ea3b493a-ba4f-4a89-d548-06fe80d9028b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100テンソルの計算時間: 0.0134秒\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "# モデルをロード\n",
        "model_path = \"/content/model2.safetensors\"\n",
        "model = load_file(model_path)\n",
        "\n",
        "# テンソルごとの推論速度測定\n",
        "start_time = time.time()\n",
        "for i, (k, v) in enumerate(model.items()):\n",
        "    _ = v * 2  # ダミー計算\n",
        "    if i >= 100:\n",
        "        break\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\" 100テンソルの計算時間: {end_time - start_time:.4f}秒\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-c-A4FsF0Qh",
        "outputId": "975018e9-f3e0-4bfb-8dc3-489e1ce2158c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100テンソルの計算時間: 0.0113秒\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def create_tokenizer_config():\n",
        "    config = {\n",
        "        \"tokenizer_class\": \"AutoTokenizer\",\n",
        "        \"model_max_length\": 512,\n",
        "        \"padding_side\": \"right\",\n",
        "        \"truncation_side\": \"right\",\n",
        "        \"special_tokens_map\": {\n",
        "            \"bos_token\": \"<s>\",\n",
        "            \"eos_token\": \"</s>\",\n",
        "            \"unk_token\": \"<unk>\",\n",
        "            \"pad_token\": \"<pad>\",\n",
        "            \"mask_token\": \"<mask>\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    os.makedirs(\"/content/sample_data\", exist_ok=True)\n",
        "    with open(\"/content/sample_data/tokenizer_config.json\", \"w\") as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    print(\" tokenizer_config.json を作成しました\")\n",
        "\n",
        "create_tokenizer_config()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLHOyQe7HrJz",
        "outputId": "537ee421-d225-4ed1-a49e-d75a52b32679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " tokenizer_config.json を作成しました\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget -P /content/sample_data https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ehDZoM7ICFx",
        "outputId": "da7ec197-7f66-473c-9312-55a03d4dd02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-17 10:13:18--  https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.121, 13.35.202.97, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231508 (226K) [text/plain]\n",
            "Saving to: ‘/content/sample_data/vocab.txt.1’\n",
            "\n",
            "vocab.txt.1         100%[===================>] 226.08K  1.03MB/s    in 0.2s    \n",
            "\n",
            "2025-02-17 10:13:18 (1.03 MB/s) - ‘/content/sample_data/vocab.txt.1’ saved [231508/231508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time\n",
        "\n",
        "# モデル＆トークナイザーをロード（tokenizerをmodelパスから自動取得）\n",
        "model_path = \"/content/merged_model.safetensors\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Define torch_dtype\n",
        "torch_dtype = torch.float16\n",
        "\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "y3kyg8gENU1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch datasets scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhXorZDwOcBw",
        "outputId": "7a62147b-79db-49f2-f004-33dcf8c98388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\"BertForSequenceClassification\"],\n",
        "    \"model_type\": \"bert\",\n",
        "    \"num_labels\": 2,\n",
        "    \"hidden_size\": 768,\n",
        "    \"num_attention_heads\": 12,\n",
        "    \"num_hidden_layers\": 12,\n",
        "    \"vocab_size\": 30522\n",
        "}\n",
        "\n",
        "with open(\"/content/sample_data/config.json\", \"w\") as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "\n",
        "print(\"✅ config.json を作成しました\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LrE0-M8RDQf",
        "outputId": "5d8ebf40-f9da-4211-8bb1-7c6492026f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ config.json を作成しました\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "\n",
        "# モデルのディレクトリパス\n",
        "model_dir = \"/content/merged_model.safetensors\"\n",
        "\n",
        "# トークナイザーのロード\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# モデルの構成情報を作成\n",
        "num_labels = 2  # 例: バイナリ分類の場合\n",
        "config = AutoConfig.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "model = AutoModelForSequenceClassification.from_config(config, torch_dtype=torch.float16)\n",
        "\n",
        "# safetensorsファイルをロードして、キーの修正\n",
        "state_dict = load_file(model_dir)\n",
        "\n",
        "# キー名修正（prefixの不一致を解消）\n",
        "corrected_state_dict = {}\n",
        "for key, value in state_dict.items():\n",
        "    # 'encoder.'が必要な場合などに対応\n",
        "    new_key = key.replace(\"encoder.block\", \"bert.encoder.layer\")\n",
        "    new_key = new_key.replace(\"embeddings\", \"bert.embeddings\")\n",
        "    new_key = new_key.replace(\"decoder\", \"bert\")\n",
        "    corrected_state_dict[new_key] = value\n",
        "\n",
        "# モデルに修正済みのstate_dictをロード\n",
        "model.load_state_dict(corrected_state_dict, strict=False)\n",
        "\n",
        "# デバイスの設定（GPUが利用可能ならGPUを使用）\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\" モデルのロードが完了しました！\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GNNnyY0P2SH",
        "outputId": "9ae8bc17-de20-4521-cdbe-668e86a5c1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " モデルのロードが完了しました！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovCNCp5rQww6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論テスト\n",
        "input_text = \"The future of AI is\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 推論実行\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "print(f\" 推論結果: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdj6QmpuQlOH",
        "outputId": "cd7ba0ce-8424-44a9-d56a-763ead1ac276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 推論結果: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論テスト：出力内容の確認\n",
        "input_text = \"The future of AI is\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 推論実行\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# 出力内容を確認\n",
        "print(f\"モデル出力の内容: {outputs}\")\n",
        "\n",
        "# logitsが存在する場合、推論結果を取得\n",
        "if hasattr(outputs, \"logits\"):\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "    print(f\" 推論結果: {predicted_class}\")\n",
        "else:\n",
        "    print(\" `logits`が出力に含まれていません\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqlUi9tbQxWX",
        "outputId": "e0e4416a-a011-4bc9-c6a9-c506217ce86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデル出力の内容: SequenceClassifierOutput(loss=None, logits=tensor([[0.2551, 0.0775]], device='cuda:0', dtype=torch.float16), hidden_states=None, attentions=None)\n",
            " 推論結果: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nX9ZdZeCQ7EF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}